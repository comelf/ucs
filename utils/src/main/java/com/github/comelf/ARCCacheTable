package com.github.comelf;

import java.util.Enumeration;
import java.util.HashMap;
import java.util.Map;
import java.util.NoSuchElementException;

/**
 * ARC (Adaptive Replacement Cache) Algorithm Implementation
 * <p>
 * ARC uses 4 LRU lists:
 * - T1: Recently referenced once (recency)
 * - T2: Recently referenced two or more times (frequency)
 * - B1: History of pages evicted from T1 (ghost entries)
 * - B2: History of pages evicted from T2 (ghost entries)
 * <p>
 * Adaptive parameter p:
 * - Dynamically adjusts the target size of T1
 * - Increases p on B1 hit (emphasizes recency)
 * - Decreases p on B2 hit (emphasizes frequency)
 *
 * @param <K> Key type
 * @param <V> Value type
 */
public class ARCCacheTable<K, V> {
    private static final int DEFAULT_CAPACITY = 101;
    private static final float DEFAULT_LOAD_FACTOR = 0.75f;

    private CacheEntry<K, V>[] table;
    private int threshold;
    private float loadFactor;

    // Total cache size
    private int maxSize;

    // Adaptive parameter p: target size of T1
    private int p;

    // T1: Recently referenced once (LRU)
    private final CacheEntry<K, V> t1Header;
    private int t1Size;

    // T2: Recently referenced two or more times (LRU)
    private final CacheEntry<K, V> t2Header;
    private int t2Size;

    // B1: History of keys evicted from T1 (ghost entries)
    private final LinkedHashMap<K, K> b1Map;
    private int b1Size;

    // B2: History of keys evicted from T2 (ghost entries)
    private final LinkedHashMap<K, K> b2Map;
    private int b2Size;

    private long defaultKeepTime = 0;

    public ARCCacheTable() {
        this(DEFAULT_CAPACITY, DEFAULT_LOAD_FACTOR);
    }

    public ARCCacheTable(int initCapacity, float loadFactor) {
        if (initCapacity < 0)
            throw new RuntimeException("Capacity Error: " + initCapacity);
        if (loadFactor <= 0)
            throw new RuntimeException("Load Count Error: " + loadFactor);
        if (initCapacity == 0)
            initCapacity = 1;

        this.loadFactor = loadFactor;
        this.table = new CacheEntry[initCapacity];
        this.threshold = (int) (initCapacity * loadFactor);

        this.t1Header = new CacheEntry<>(null, null, 0, null, ListType.T1);
        this.t1Header.link_next = this.t1Header.link_prev = this.t1Header;

        this.t2Header = new CacheEntry<>(null, null, 0, null, ListType.T2);
        this.t2Header.link_next = this.t2Header.link_prev = this.t2Header;

        this.b1Map = new LinkedHashMap<>();
        this.b2Map = new LinkedHashMap<>();
    }

    public ARCCacheTable<K, V> setMaxRow(int max) {
        if (max <= 0) {
            throw new IllegalArgumentException("maxSize <= 0");
        }
        this.maxSize = max;
        this.p = 0;  // Initially equal distribution between T1/T2
        return this;
    }

    public ARCCacheTable<K, V> setDefaultKeepTime(long time) {
        this.defaultKeepTime = time;
        return this;
    }

    public int size() {
        return t1Size + t2Size;
    }

    public synchronized V get(K key) {
        if (key == null)
            return null;

        CacheEntry<K, V> e = getEntry(key);
        if (e == null) {
            return null;
        }

        if (e.isExpired()) {
            removeEntry(e);
            return null;
        }

        // ARC algorithm: promote from T1 to T2 if in T1
        promoteToT2(e);

        return e.getValue();
    }

    public synchronized V getKeepAlive(K key, long keepAlive) {
        if (key == null)
            return null;

        CacheEntry<K, V> e = getEntry(key);
        if (e == null)
            return null;

        if (e.isExpired()) {
            removeEntry(e);
            return null;
        }

        e.keepAlive(keepAlive);
        promoteToT2(e);

        return e.getValue();
    }

    public synchronized V getKeepAlive(K key) {
        return getKeepAlive(key, defaultKeepTime);
    }

    private CacheEntry<K, V> getEntry(K key) {
        if (key == null)
            return null;

        CacheEntry<K, V>[] tab = table;
        int index = hash(key) % tab.length;

        for (CacheEntry<K, V> e = tab[index]; e != null; e = e.next) {
            if (CompareUtil.equals(e.key, key)) {
                return e;
            }
        }
        return null;
    }

    public synchronized V put(K key, V value) {
        return put(key, value, defaultKeepTime);
    }

    public synchronized V put(K key, V value, long keepTime) {
        if (key == null)
            return null;

        CacheEntry<K, V> existing = getEntry(key);
        if (existing != null) {
            V old = existing.value;
            existing.value = value;
            existing.keepAlive(keepTime);
            promoteToT2(existing);
            return old;
        }

        // Check if was in B1 or B2 (ghost entry)
        boolean inB1 = isInB1(key);
        boolean inB2 = isInB2(key);

        if (inB1) {
            // B1 hit: recency important -> increase p
            adaptP(true);
            removeFromB1(key);

            // Secure space then insert directly to T2
            if (size() >= maxSize) {
                replace(ListType.B1);
            }
            insertToT2(key, value, keepTime);
        } else if (inB2) {
            // B2 hit: frequency important -> decrease p
            adaptP(false);
            removeFromB2(key);

            // Secure space then insert directly to T2
            if (size() >= maxSize) {
                replace(ListType.B2);
            }
            insertToT2(key, value, keepTime);
        } else {
            // New key: apply onMiss logic
            onMiss(key, value, keepTime);
        }

        return null;
    }

    /**
     * Adjust adaptive parameter p
     * According to ARC paper: δ = max(1, |B2| / |B1|) for B1 hit, δ = max(1, |B1| / |B2|) for B2 hit
     *
     * @param b1Hit true if B1 hit (increase p), false if B2 hit (decrease p)
     */
    private void adaptP(boolean b1Hit) {
        onAdaptation();

        if (b1Hit) {
            // B1 hit: recency pattern -> increase T1 size
            // δ = max(1, |B2| / |B1|)
            int delta = (b1Size == 0) ? 1 : Math.max(1, b2Size / b1Size);
            p = Math.min(p + delta, maxSize);
        } else {
            // B2 hit: frequency pattern -> decrease T1 size
            // δ = max(1, |B1| / |B2|)
            int delta = (b2Size == 0) ? 1 : Math.max(1, b1Size / b2Size);
            p = Math.max(p - delta, 0);
        }
    }

    /**
     * Cache replacement policy (core of ARC)
     * Caffeine ARC algorithm implementation reference
     *
     * @param candidateType Type of candidate to insert (check if from B2)
     */
    private void replace(ListType candidateType) {
        // if (|T1| ≥ 1) and ((x ∈ B2 and |T1| = p) or (|T1| > p))
        //   then move the LRU page of T1 to the top of B1 and remove it from the cache.
        // else move the LRU page in T2 to the top of B2 and remove it from the cache.

        if ((t1Size >= 1) && (((candidateType == ListType.B2) && (t1Size == p)) || (t1Size > p))) {
            evictFromT1ToB1();
        } else {
            evictFromT2ToB2();
        }
    }

    /**
     * Cache miss handling (new key)
     * Caffeine ARC onMiss algorithm implementation
     * <p>
     * x ∈ L1 ∪ L2 (a miss in DBL(2c) and ARC(c)):
     * case (i) |L1| >= c:
     * If |T1| < c then delete the LRU page of B1 and REPLACE(p).
     * else delete LRU page of T1 and remove it from the cache.
     * case (ii) |L1| < c and |L1| + |L2| >= c:
     * if |L1| + |L2| = 2c then delete the LRU page of B2.
     * REPLACE(p).
     * Put x at the top of T1 and place it in the cache.
     */
    private void onMiss(K key, V value, long keepTime) {
        int l1Size = t1Size + b1Size;  // L1 = T1 ∪ B1
        int l2Size = t2Size + b2Size;  // L2 = T2 ∪ B2

        // case (i) |L1| >= c (changed from == to handle growing L1)
        if (l1Size >= maxSize) {
            if (t1Size < maxSize) {
                // If T1 has space, remove from B1 then replace
                removeLRUFromB1();
                replace(ListType.B1);
            } else {
                // If T1 is full, evict from T1 (moved to B1)
                evictFromT1ToB1();
            }
        }
        // case (ii) |L1| < c and |L1| + |L2| >= c
        else if (l1Size + l2Size >= maxSize) {
            if ((l1Size + l2Size) >= (2 * maxSize)) {
                // If full, remove from B2
                removeLRUFromB2();
            }
            replace(ListType.B1);
        }

        // Put x at the top of T1 and place it in the cache
        insertToT1(key, value, keepTime);
    }

    private void insertToT1(K key, V value, long keepTime) {
        // onMiss already handled space management, just insert here
        if (size() >= threshold) {
            rehash();
        }

        CacheEntry<K, V>[] tab = table;
        int index = hash(key) % tab.length;

        CacheEntry<K, V> e = new CacheEntry<>(key, value, keepTime, tab[index], ListType.T1);
        tab[index] = e;

        // Add to T1 MRU position
        chain(t1Header.link_prev, t1Header, e);
        t1Size++;
    }

    private void insertToT2(K key, V value, long keepTime) {
        if (size() >= threshold) {
            rehash();
        }

        CacheEntry<K, V>[] tab = table;
        int index = hash(key) % tab.length;

        CacheEntry<K, V> e = new CacheEntry<>(key, value, keepTime, tab[index], ListType.T2);
        tab[index] = e;

        // Add to T2 MRU position
        chain(t2Header.link_prev, t2Header, e);
        t2Size++;
    }

    private void promoteToT2(CacheEntry<K, V> e) {
        if (e.listType == ListType.T2) {
            // If already in T2, move to MRU
            if (t2Header.link_prev != e) {
                unchain(e);
                chain(t2Header.link_prev, t2Header, e);
            }
        } else if (e.listType == ListType.T1) {
            // Promote from T1 to T2
            unchain(e);
            t1Size--;
            e.listType = ListType.T2;

            chain(t2Header.link_prev, t2Header, e);
            t2Size++;
        }
    }

    /**
     * Evict LRU from T1 to B1 (complete removal from cache)
     */
    protected void evictFromT1ToB1() {
        if (t1Size == 0)
            return;

        CacheEntry<K, V> victim = t1Header.link_next;
        if (victim == t1Header)
            return;

        K key = victim.key;
        V value = victim.value;

        removeFromHashTable(key);
        unchain(victim);
        t1Size--;
        onEviction();

        entryRemoved(true, key, value, null);

        // Add ghost entry to B1
        addToB1(key);
    }

    protected void onAdaptation() {
        // Default: do nothing (no overhead)
    }

    protected void onEviction() {
        // Default: do nothing (no overhead)
    }

    /**
     * Evict LRU from T2 to B2 (complete removal from cache)
     */
    protected void evictFromT2ToB2() {
        if (t2Size == 0)
            return;

        CacheEntry<K, V> victim = t2Header.link_next;
        if (victim == t2Header)
            return;

        K key = victim.key;
        V value = victim.value;

        removeFromHashTable(key);
        unchain(victim);
        t2Size--;
        onEviction();

        entryRemoved(true, key, value, null);

        // Add ghost entry to B2
        addToB2(key);
    }

    private void addToB1(K key) {
        b1Map.put(key, key);
        b1Size++;
    }

    private void addToB2(K key) {
        b2Map.put(key, key);
        b2Size++;
    }

    private boolean isInB1(K key) {
        return b1Map.containsKey(key);
    }

    private boolean isInB2(K key) {
        return b2Map.containsKey(key);
    }

    private void removeFromB1(K key) {
        if (b1Map.remove(key) != null) {
            b1Size--;
        }
    }

    private void removeFromB2(K key) {
        if (b2Map.remove(key) != null) {
            b2Size--;
        }
    }

    private void removeLRUFromB1() {
        if (b1Size == 0)
            return;

        K oldestKey = b1Map.keySet().iterator().next();
        b1Map.remove(oldestKey);
        b1Size--;
    }

    private void removeLRUFromB2() {
        if (b2Size == 0)
            return;

        K oldestKey = b2Map.keySet().iterator().next();
        b2Map.remove(oldestKey);
        b2Size--;
    }

    private void removeFromHashTable(K key) {
        if (key == null)
            return;

        CacheEntry<K, V>[] tab = table;
        int index = hash(key) % tab.length;

        for (CacheEntry<K, V> e = tab[index], prev = null; e != null; prev = e, e = e.next) {
            if (CompareUtil.equals(e.key, key)) {
                if (prev != null) {
                    prev.next = e.next;
                } else {
                    tab[index] = e.next;
                }
                return;
            }
        }
    }

    private void removeEntry(CacheEntry<K, V> e) {
        removeFromHashTable(e.key);
        unchain(e);

        if (e.listType == ListType.T1) {
            t1Size--;
        } else if (e.listType == ListType.T2) {
            t2Size--;
        }
    }

    public synchronized V remove(K key) {
        if (key == null)
            return null;

        CacheEntry<K, V> e = getEntry(key);
        if (e == null)
            return null;

        V value = e.value;
        removeEntry(e);

        removeFromB1(key);
        removeFromB2(key);

        return value;
    }

    public synchronized void clear() {
        CacheEntry<K, V>[] tab = table;
        for (int index = tab.length; --index >= 0; )
            tab[index] = null;

        t1Header.link_next = t1Header.link_prev = t1Header;
        t2Header.link_next = t2Header.link_prev = t2Header;

        b1Map.clear();
        b2Map.clear();

        t1Size = 0;
        t2Size = 0;
        b1Size = 0;
        b2Size = 0;
        p = 0;
    }

    public boolean isEmpty() {
        return size() == 0;
    }

    public synchronized boolean containsKey(K key) {
        return getEntry(key) != null;
    }

    public synchronized Enumeration<K> keys() {
        return new Enumer<K>(TYPE.KEYS);
    }

    public synchronized Enumeration<V> values() {
        return new Enumer<V>(TYPE.VALUES);
    }

    public synchronized Enumeration<CacheEntry<K, V>> entries() {
        return new Enumer<CacheEntry<K, V>>(TYPE.ENTRIES);
    }

    private int hash(Object key) {
        return (key.hashCode()) & Integer.MAX_VALUE;
    }

    protected void rehash() {
        int oldCapacity = table.length;
        CacheEntry<K, V>[] oldMap = table;
        int newCapacity = oldCapacity * 2 + 1;
        CacheEntry<K, V>[] newMap = new CacheEntry[newCapacity];

        threshold = (int) (newCapacity * loadFactor);
        table = newMap;

        for (int i = oldCapacity; i-- > 0; ) {
            for (CacheEntry<K, V> old = oldMap[i]; old != null; ) {
                CacheEntry<K, V> e = old;
                old = old.next;
                K key = e.key;
                int index = hash(key) % newCapacity;
                e.next = newMap[index];
                newMap[index] = e;
            }
        }
    }

    public String getStats() {
        return String.format("ARCCache[total=%d/%d, T1=%d, T2=%d, B1=%d, B2=%d, p=%d]",
                size(), maxSize, t1Size, t2Size, b1Size, b2Size, p);
    }

    private void chain(CacheEntry<K, V> link_prev, CacheEntry<K, V> link_next, CacheEntry<K, V> e) {
        e.link_prev = link_prev;
        e.link_next = link_next;
        link_prev.link_next = e;
        link_next.link_prev = e;
    }

    private void unchain(CacheEntry<K, V> e) {
        e.link_prev.link_next = e.link_next;
        e.link_next.link_prev = e.link_prev;
        e.link_prev = null;
        e.link_next = null;
    }

    protected void entryRemoved(boolean evicted, K key, V oldValue, V newValue) {
        // Default implementation is empty. Override in subclass
    }

    public synchronized void resize(int maxSize) {
        if (maxSize <= 0) {
            throw new IllegalArgumentException("maxSize <= 0");
        }

        this.maxSize = maxSize;
        this.p = 0;

        Map<K, V> snapshot = snapshot();
        clear();

        for (Map.Entry<K, V> entry : snapshot.entrySet()) {
            put(entry.getKey(), entry.getValue());
        }
    }

    public synchronized Map<K, V> snapshot() {
        Map<K, V> result = new HashMap<>();

        CacheEntry<K, V> current = t1Header.link_next;
        while (current != t1Header) {
            result.put(current.key, current.value);
            current = current.link_next;
        }

        current = t2Header.link_next;
        while (current != t2Header) {
            result.put(current.key, current.value);
            current = current.link_next;
        }

        return result;
    }


    public synchronized int getP() {
        return p;
    }

    public synchronized int maxSize() {
        return maxSize;
    }

    protected int getT1Size() {
        return t1Size;
    }

    protected int getT2Size() {
        return t2Size;
    }

    protected int getB1Size() {
        return b1Size;
    }

    protected int getB2Size() {
        return b2Size;
    }

    @Override
    public synchronized String toString() {
        return String.format("ARCCache[size=%d/%d, T1=%d, T2=%d, B1=%d, B2=%d, p=%d]",
                size(), maxSize, t1Size, t2Size, b1Size, b2Size, p);
    }

    private enum ListType {
        T1, T2, B1, B2
    }

    public static class CacheEntry<K, V> {
        private K key;
        private V value;
        private long timeOfExpiration;
        private long keepTime = 0;
        private ListType listType;

        CacheEntry<K, V> next;
        CacheEntry<K, V> link_next, link_prev;

        protected CacheEntry(K key, V value, long keepTime, CacheEntry<K, V> next, ListType listType) {
            this.key = key;
            this.value = value;
            this.listType = listType;
            this.keepAlive(keepTime);
            this.next = next;
        }

        public K getKey() {
            return key;
        }

        public V getValue() {
            return value;
        }

        public V setValue(V value) {
            V oldValue = this.value;
            this.value = value;
            return oldValue;
        }

        public boolean isExpired() {
            if (timeOfExpiration > 0) {
                return timeOfExpiration < DateUtil.currentTime();
            }
            return false;
        }

        public void keepAlive(long keepTime) {
            if (keepTime > 0) {
                this.keepTime = keepTime;
                this.timeOfExpiration = DateUtil.currentTime() + keepTime;
            } else {
                this.keepTime = 0;
                this.timeOfExpiration = 0;
            }
        }

        public String toString() {
            return key + "=" + value + "[" + listType + "]";
        }
    }

    private enum TYPE {
        KEYS, VALUES, ENTRIES
    }

    private class Enumer<T> implements Enumeration<T> {
        TYPE type;
        CacheEntry<K, V> t1Entry;
        CacheEntry<K, V> t2Entry;
        boolean inT1 = true;

        Enumer(TYPE type) {
            this.type = type;
            this.t1Entry = ARCCacheTable.this.t1Header.link_next;
            this.t2Entry = ARCCacheTable.this.t2Header.link_next;
        }

        public boolean hasMoreElements() {
            if (inT1 && t1Entry != t1Header) {
                return true;
            }
            inT1 = false;
            return t2Entry != t2Header && t2Entry != null;
        }

        public T nextElement() {
            if (!hasMoreElements()) {
                throw new NoSuchElementException("no more next");
            }

            CacheEntry<K, V> e;
            if (inT1) {
                e = t1Entry;
                t1Entry = e.link_next;
            } else {
                e = t2Entry;
                t2Entry = e.link_next;
            }

            switch (type) {
                case KEYS:
                    return (T) e.key;
                case VALUES:
                    return (T) e.value;
                default:
                    return (T) e;
            }
        }
    }
}
